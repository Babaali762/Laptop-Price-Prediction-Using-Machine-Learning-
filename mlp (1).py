# -*- coding: utf-8 -*-
"""mlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hVa8UwWrHPM8T_c2izZt798Hh4b-OxTN

1) IMPORT LIBRARIES
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""2) DATA LOADING AND UNDERSTANDING"""

df=pd.read_csv("/content/Laptop_price.csv")

df

df.head()

df.tail()

df.info()

df.describe()

df.columns

df.isnull().sum()

sns.countplot(data=df, x='Brand', hue='RAM_Size')
plt.title('Ram size by brand')
plt.xlabel('Brand')
plt.ylabel('Ram_size')
plt.legend(title='Ram__Size')
plt.show()

Ram_count = df['RAM_Size'].value_counts()
# Pie chart
plt.figure(figsize=(6,6))
plt.pie(Ram_count, labels=Ram_count.index, autopct='%1.1f%%', startangle=90, colors=['#66b3ff','#ff9999'])
plt.title('Ram Size')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

sns.boxplot(data=df, x='Brand', y='Screen_Size')
plt.title('Screen Size')
plt.show()

from sklearn.preprocessing import LabelEncoder

sns.countplot(data=df, x= 'Brand', hue='Storage_Capacity')
plt.title('Stoorage capacity')
plt.xlabel('Brand')
plt.ylabel('Storage')
plt.legend(title='Ram capacity')
plt.show()

from sklearn.preprocessing import LabelEncoder

# Columns to encode
cat_cols = ['Brand']

le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col].astype(str))

df

from sklearn.preprocessing import StandardScaler

# Columns to scale
num_cols = ['Price','Storage_Capacity','RAM_Size']

# Apply StandardScaler
scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

# Save back to the same CSV file
df.to_csv('loan_dataset.csv', index=False)

df

# Calculate correlation matrix
corr_matrix = df.corr(numeric_only=True)

# Focus on correlation with the target variable
target_corr = corr_matrix[['Price']].sort_values(by='Price', ascending=False)

# Plot heatmap
plt.figure(figsize=(6,8))
sns.heatmap(target_corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Feature Correlation with Price Prediction')
plt.show()

# Separate features and target
X = df.drop(['Price'], axis=1) # Pass columns to drop as a list
y = df['Price']

from sklearn.model_selection import train_test_split

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Decision Tree

"""

from sklearn.tree import DecisionTreeRegressor

# Fit the Decision Tree model
model_dt = DecisionTreeRegressor()
model_dt.fit(X_train, y_train)

# Predict
y_pred = model_dt.predict(X_test)

print(y_pred)

from sklearn.metrics import mean_squared_error, r2_score

y_pred = model_dt.predict(X_test)

# Evaluate using regression metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

# Assuming X and y are defined
model = DecisionTreeRegressor()
model.fit(X, y)  # Fit on your training data (X_train, y_train)

# Get feature importances
importances = model.feature_importances_

# Instead of X.columns, use the original feature names
# or column names from the DataFrame before conversion:
feature_names = df.drop(['Price'], axis=1).columns  # Assuming 'df' is your original DataFrame

# Create a DataFrame
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel("Feature Importance")
plt.title("Decision Tree Feature Importance")
plt.gca().invert_yaxis()
plt.show()

from sklearn.metrics import r2_score

# Assuming y_test contains your true target values
r2 = r2_score(y_test, y_pred)
print("R² Score (Accuracy-like):", r2)

"""Random Forest"""

from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor()
model_rf.fit(X_train, y_train)

# Predict
y_pred = model_rf.predict(X_test)

# Predict
y_pred = model_dt.predict(X_test)

y_pred

y_pred = model_dt.predict(X_test)

# Evaluate using regression metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

"""Randon forest"""

# Fit the Decision Tree model
model_dt = RandomForestRegressor()
model_dt.fit(X_train, y_train)

from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import matplotlib.pyplot as plt

# Assuming X and y are defined
model = RandomForestRegressor()
model.fit(X, y)

# Get feature importances
importances = model.feature_importances_
feature_names = X.columns

# Create a DataFrame
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel("Feature Importance")
plt.title("Random Forest Feature Importance")
plt.gca().invert_yaxis()
plt.show()

y_pred = model_dt.predict(X_test)

# Evaluate using regression metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

r2 = r2_score(y_test, y_pred)
print("R² Score (Accuracy-like):", r2)

""" Linear Regression"""

from sklearn.linear_model import LinearRegression

# Fit the Decision Tree model
model_dt = LinearRegression()
model_dt.fit(X_train, y_train)

y_pred = model_dt.predict(X_test)

# Evaluate using regression metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

r2 = r2_score(y_test, y_pred)
print("R² Score (Accuracy-like):", r2)

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, r2_score # Import accuracy_score and r2_score
from sklearn.linear_model import LinearRegression # Import LinearRegression
from sklearn.tree import DecisionTreeRegressor # Import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor # Import RandomForestRegressor
from sklearn.svm import SVR # Import SVR

# ... (your previous code to load and preprocess data: X_train, X_test, y_train, y_test) ...

# Re-initialize and train the models
model_dt = DecisionTreeRegressor()  # Decision Tree
model_dt.fit(X_train, y_train)

model_lr = LinearRegression()  # Linear Regression
model_lr.fit(X_train, y_train)

model_rf = RandomForestRegressor()  # Random Forest
model_rf.fit(X_train, y_train)

model_svr = SVR()  # SVR
model_svr.fit(X_train, y_train)

# Calculate accuracies (using r2_score for regression problems)
y_pred_dt = model_dt.predict(X_test)
y_pred_lr = model_lr.predict(X_test)  # Predictions from Linear Regression
y_pred_rf = model_rf.predict(X_test)


accuracy_dt = r2_score(y_test, y_pred_dt)
accuracy_lr = r2_score(y_test, y_pred_lr)  # Calculate R-squared for Linear Regression
accuracy_rf = r2_score(y_test, y_pred_rf)


# Data for the bar graph
models = ['Decision Tree', 'Linear Regression', 'Random Forest']
accuracies = [accuracy_dt, accuracy_lr, accuracy_rf]

# Creating the bar graph
plt.figure(figsize=(10, 6))
bars = plt.bar(models, accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'lightsalmon'])

# Add accuracy values on top of the bars
plt.bar_label(bars, labels=[str(round(v, 2)) for v in accuracies])

# Add labels and title
plt.xlabel("Models")
plt.ylabel("Accuracy (R-squared)")  # Change y-axis label to "Accuracy (R-squared)"
plt.title("Accuracy Comparison of Different Models")

plt.show()

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py

!wget -q -O - ipv4.icanhazip.com

! streamlit run app.py & npx localtunnel --port 8501